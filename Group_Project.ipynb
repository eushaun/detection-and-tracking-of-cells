{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9517: Computer Vision, 2020 Term 2\n",
    "## Group Project \n",
    "### Detecting and Tracking of Cells in Time-Lapse Microscopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "import cv2 as cv\n",
    "\n",
    "# Import Detector\n",
    "from preprocessing.preprocess import *\n",
    "from preprocessing.model import *\n",
    "from detection import *\n",
    "\n",
    "# Import Tracker\n",
    "from tracker import *\n",
    "\n",
    "# Mouse event detection\n",
    "mouseX = mouseY = 0\n",
    "\n",
    "def detect_mouse(event,x,y,flags,param):\n",
    "    global mouseX, mouseY\n",
    "    if event == cv.EVENT_LBUTTONDOWN:\n",
    "        mouseX,mouseY = x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Setup\n",
    "The directory for this notebook should include the:\n",
    "- This notebook 'Group_Project.ipynb'\n",
    "- The sub-directory preprocessing, this includes the Unet Model used for detection, the preprocessing steps and the saved model weights\n",
    "- The detection.py and tracker.py file\n",
    "- The sub-directory 'inputs' that includes the files extracted from the 'COMP9517 20T2 Group Project Image Sequences v2.zip'\n",
    "\n",
    "### User inputs\n",
    "Set the f_flag value to the required data set.\n",
    "0 - DIC-C2DH-HeLa\n",
    "1 - Fluo-N2DL-HeLa\n",
    "2 - PhC-C2DL-PSC\n",
    "\n",
    "and sequence to the sequence number required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of files: 426\nFirst File: t000.tif\n"
     ]
    }
   ],
   "source": [
    "# Global parameters\n",
    "f_flag = 2\n",
    "sequence = 1  \n",
    "\n",
    "if f_flag==0:\n",
    "    path = f\"input/DIC-C2DH-HeLa/Sequence {sequence}\"\n",
    "elif f_flag==1:\n",
    "    path = f\"input/Fluo-N2DL-HeLa/Sequence {sequence}\"\n",
    "else:\n",
    "    path = f\"input/PhC-C2DL-PSC/Sequence {sequence}\"\n",
    "    \n",
    "# Image sequences \n",
    "im_files = [f for f in os.listdir(path) if f[-3:] == 'tif']\n",
    "print(\"Number of files: %d\\nFirst File: %s\" % (len(im_files), im_files[0]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Detect and Track Cells\n",
    "### 1.1 Cell Detection\n",
    "1. Preprocess training data and store in .../training\n",
    "2. Augment training data and feed into unet model\n",
    "3. Train model\n",
    "4. Segment cells using unet for DIC-C2DH-HeLa\n",
    "5. Use other preprocessing techniques for Fluo-N2DL-HeLa and PhC-C2DL-PSC\n",
    "6. Draw bounding boxes for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and preprocess training images\n",
    "# need to create the folders beforehand, if not it won't write\n",
    "# training images are stored in {path}/training/image\n",
    "# masks are stored in {path}/training/image \n",
    "create_training(path, f_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "# U-Net model\n",
    "model = unet()\n",
    "\n",
    "# Load previously trained model\n",
    "model.load_weights('preprocessing/unet_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cell Tracking\n",
    "\n",
    "CentroidTracker, is a simple tracking algorithm that takes the input of a list of bounding boxes and calculates the centroid (cX,cY), and tries to match the closes object in the next frame. The output is a list of object centroids and the trajectory based on previous frames. \n",
    "\n",
    "ExtTracker, is an extension of Centroid Tracker that will calculate the nearest neighbour based on the centroid and 3 additional features, these features have and adjustable weight that can be set when initialising to prioritise particular features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalise tracker\n",
    "tracker = CentroidTracker()\n",
    "trackerType = 0\n",
    "# initalise boxes input \"boxes\" list of bounding boxes (startX, startY, endX, endY)\n",
    "boxes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalise tracker with weights for each feature\n",
    "tracker = ExtTracker(alpha1 = 0.01, alpha2 = 0.01, alpha3 = 0.01)\n",
    "trackerType = 1\n",
    "# initalise boxes input \"shapes\" list of bounding boxes and features (startX, startY, endX, endY, Feature1, Feature2, Feature3)\n",
    "shapes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Analysing Cell Motion\n",
    "We can calculate the speed, total distance, net distance and subsequent confinement ratio of each cell by simply left-clicking on the selected cell and then pressing the 'a' key on your keyboard. The program should then print out the results everytime the 'a' key is pressed. To select a new cell, left-click on the new cell and press the 'a' key again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for f in im_files:\n",
    "    #Read each file in and copy to frame for processing and output\n",
    "    im = cv.imread(\"%s/%s\" % (path, f))\n",
    "    frame = im.copy()\n",
    "    \n",
    "    # detect the cells and features from the frame \n",
    "    # 'contours' is the sell outline used for motosis detection and feature extraction\n",
    "    # 'boxes' is a list to bounding boxes fro each cell in the frame\n",
    "    contours, boxes = detect(frame, f_flag, model)\n",
    "    frame = c_stretch(frame, 0, 255)\n",
    "    \n",
    "    features = get_features(frame, contours)\n",
    "\n",
    "    text = \"Number of Cells Detected: {}\".format(len(boxes))\n",
    "    cv.putText(frame, text, (20, 20), cv.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1)\n",
    "    \n",
    "    # cumulative distribution function used for mitosis deection \n",
    "    cdf = get_cdf(frame, f_flag)\n",
    "    mitosis_count = 0\n",
    "    \n",
    "    for cnt in contours:\n",
    "        if (is_mitosis(frame, f_flag, cdf, cnt)):\n",
    "            colour = (0, 255, 0)\n",
    "            mitosis_count += 1\n",
    "        else:\n",
    "            colour = (0, 255, 255)\n",
    "        x, y, w, h = cv.boundingRect(cnt)\n",
    "        cv.rectangle(frame, (x, y), (x+w, y+h), colour, 1)\n",
    "        \n",
    "    text = \"Number of Mitoses Detected: {}\".format(mitosis_count)\n",
    "    cv.putText(frame, text, (20, 40), cv.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1)\n",
    "    \n",
    "    if trackerType: \n",
    "        #If extended tracker input boxes needs to be in the form, [(x1, y1, x2, y2, F1, F2, F3)]\n",
    "        # Place holder features \n",
    "        objects, trajectorys = tracker.update(features)\n",
    "    else: \n",
    "        #If Centroid Tracker \n",
    "        objects, trajectorys = tracker.update(boxes)\n",
    "    \n",
    "    # loop over the tracked objects, trajectory is a list of all the previous object centres [(cXn-1, cYn-1), (cXn, cYn)]\n",
    "    for (objectID, trajectory) in trajectorys.items():\n",
    "        if len(trajectory) > 1:            \n",
    "            for i in range(1, len(trajectory)):\n",
    "                x1y1 = (trajectory[i-1][0], trajectory[i-1][1])\n",
    "                x2y2 = (trajectory[i][0], trajectory[i][1])\n",
    "                cv.line(frame, x1y1, x2y2, (255, 0, 0), 1)\n",
    "\n",
    "    # show the output frame\n",
    "    cv.imshow(\"Frame\", frame)\n",
    "    cv.setMouseCallback('Frame', detect_mouse)\n",
    "    k = cv.waitKey(1)\n",
    "    # pressing the esc key will break the loop\n",
    "    if k == 27:\n",
    "        break\n",
    "    # left clicking anywhere in the image and pressing 'a' will print the coordinates and the analysys of the nearest cell\n",
    "    elif k == ord('a'):\n",
    "        print(f\"Clicked at: ({mouseX}, {mouseY})\")\n",
    "        Speed, TotalDistance, NetDistance, ConfinementRatio = tracker.analyse((mouseX, mouseY))\n",
    "        print(\"Speed: %.3f pixels per frame\\nTotalDistance: %.3f pixels\\nNetDistance: %.3f pixels\\nConfinementRatio: %.3f\\n\" %\n",
    "              (Speed, TotalDistance, NetDistance, ConfinementRatio))\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debug to force close window on error\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Model develpment and training for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 0 images belonging to 1 classes.\n",
      "Found 0 images belonging to 1 classes.\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "# Part of U-net training section\n",
    "## Additional parameters to mess around for augmentation:\n",
    "target_size = (256,256)\n",
    "batch_size = 32\n",
    "rotation_range = 30\n",
    "zoom_range = 0.15\n",
    "width_shift_range = 0.2\n",
    "height_shift_range = 0.2\n",
    "shear_range = 0.15\n",
    "\n",
    "## Augment training data\n",
    "training_generator = training_augmentation(path, target_size, batch_size, rotation_range, zoom_range, \n",
    "                                            width_shift_range, height_shift_range, shear_range)\n",
    "# train new model\n",
    "# model_checkpoint = ModelCheckpoint(\n",
    "#     'unet_model1.hdf5',\n",
    "#     monitor='loss',\n",
    "#     save_best_only=True\n",
    "# )\n",
    "\n",
    "# import tensorflow as tf\n",
    "# with tf.device(\"cpu:0\"):\n",
    "#     model.fit(training_generator, steps_per_epoch=300, epochs=2, callbacks=[model_checkpoint])\n",
    "\n",
    "# U-Net model\n",
    "model = unet()\n",
    "\n",
    "# or load previously trained model\n",
    "model.load_weights('preprocessing/unet_model.hdf5') # This is still funky, i gotta touch up on the unet\n",
    "\n",
    "# predict mask, draw bounding box and output final images\n",
    "# output is stored in output/{path}/Sequence [1-4]/txxx.tif\n",
    "# make sure the folder exists in your computer first, \n",
    "# idk why it doesn't auto create it\n",
    "cell_detection(path, f_flag, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}